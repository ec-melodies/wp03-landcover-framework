#######################################################
# 
# webpageAccess.py
# Python implementation of the Class WebpageAccess
# Generated by Enterprise Architect
# Created on:      24-May-2016 12:58:37
# Original author: Jane
# 
#######################################################
import urllib2
import base64
import cookielib
import os

"""
NAME
    webpageAccess - implementation of class
FILE
    webpageAccess.py
CLASSES
    WebpageAccess
"""

from GetData.src import LPDAAC_website as website_const


class WebpageAccess:
    # TODO Allow product to be specified in different ways (Terra/Aqua/MOD/MYD/combined).
    """
    Class to retrieve data from archive.

    Methods defined here:
        set_config_object(...)
            Set up access to a configuration class instance which contains all the details needed.

        download_data_files(...)
            Retrieves all data files specified.

        scrape_web_page(...)
            Copy contents of page at given address and save to text file.

        retrieve_data_files(...)
            Download required data from archive website.

    ----------------------------------------------------------------------
    No data or other attributes defined here.

    """

    def __init__(self):
        """
        Set up a temporary file to contain web page text
        :return: no return
        """
        self.m_web_text_file_name = 'scrape.txt'
        self.m_config = None
        self.m_test_mode = 'No'

    def set_config_object(self, config):
        """
        Set up access to a configuration class instance which contains all the details needed.
        Must be done prior to any web-page access.

        :param config: a configuration:Configuration instance
        :return: no return
        """
        self.m_config = config

    def download_data_files(self, test_mode):
        """
        Retrieves all data files specified.

        :param test_mode: controls webpage access for testing purposes.
        :return: no return
        """
        # set up the correct web page dependent on mode
        self.m_test_mode = test_mode
        if test_mode == 'Git':
            parent_web_page = "http://ec-melodies.github.io/wp03-landcover-framework/"
        else:
            parent_web_page = self.m_config.create_URL()

        # get the contents of the parent web page
        if self.m_config.is_valid_day():
            self.scrape_web_page(parent_web_page)

        # while DoY not at the finish
        while self.m_config.is_valid_day():
            # convert the args to filename strings for data and xml - use config class
            local_filenames = self.m_config.create_local_filenames()

            # check whether the files have already been downloaded
            for target in local_filenames:
                if os.path.isfile(target):
                    print("File already exists {}".format(target))
                else:
                    self.retrieve_data_files(parent_web_page, self.m_config.get_tile(), local_filenames)

            # increment to next DoY
            self.m_config.next_day()
        # end while

    def scrape_web_page(self, address):
        """
        Copy contents of page at given address and save to text file.

        :param address: web page to scrape
        :return: no return
        """
        # Get the file list from the pre-defined web page and put into temporary file
        self.__download_page(address, self.m_web_text_file_name)

    def retrieve_data_files(self, archive_address, tile, local_filenames):
        """
        Download required data from archive website.

        + web address is assumed to be that of the archive
        + the tile must be valid in that it appears in the filename of available data
        + the destination filename will uniquely identify the data.

        :param archive_address: archive web address i.e. the one scraped for details of files
        :param tile: required tile
        :param filename: destination local file name
        :return: no return
        """
        if archive_address.find('git') != -1:  # could test self.m_test_mode instead...
            archive_address =\
                'https://raw.githubusercontent.com/ec-melodies/wp03-landcover-framework/gh-pages'
        # Scan the list for the right entry
        files_to_download = self.__find_file_links(tile, archive_address)

        # Download the data and xml
        if len(files_to_download) >= 1:
            # get files
            local_file_ref = 0
            for target in files_to_download:
                self.__download_page(target, local_filenames[local_file_ref])
                local_file_ref += 1
        else:
            # TODO log if no file available
            pass

    def __download_page(self, address, destination_file):
        """
        Open the URL and read contents of the web page into the destination file.

        :param address: Valid URL
        :param destination_file: target local file
        :return: no return
        """
        # This was working and then started giving the following error:
        #   HTTPError: HTTP Error 302: The HTTP server returned a redirect error that would lead to an infinite loop.
        #   The last 30x error message was:
        #   Found
        # Addition of 'CookieJar' and 'opener' are to fix.
        # See:
        #   http://stackoverflow.com/questions/9113652/how-do-i-set-cookies-using-python-urlopen
        #   http://stackoverflow.com/questions/9926023/handling-rss-redirects-with-python-urllib2
        #   http://stackoverflow.com/questions/5606083/how-to-set-and-retrieve-cookie-in-http-header-in-python
        # Also see https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+Python for a very long-winded
        # way of doing the same thing!
        try:
            if self.m_test_mode != 'Git':
                request = urllib2.Request(address)
                base64string = base64.b64encode('%s:%s' % (self.m_config.get_user(), self.m_config.get_passwd()))
                request.add_header("Authorization", "Basic %s" % base64string)
                cj = cookielib.CookieJar()
                opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
                response = opener.open(request)
            else:
                response = urllib2.urlopen(address)

            with open(destination_file, 'w') as f:
                # if we are in Real test mode and getting data (rather than the scrape web page), limit to 1000 bytes
                if self.m_test_mode == 'Real' and destination_file != self.m_web_text_file_name:
                    f.write(response.read(1000))
                else:
                    f.write(response.read())
        except urllib2.URLError as URL_err:
            print ("Problem getting web page data {}".format(URL_err.reason))


    def __find_file_links(self, tile, archive_address):
        """
        Sub-sample file list for those conforming to search criteria.

        Webpage has already been selected based on product and date, so
        we are just looking for the correct tile, and the string must
        also contain '.hdf' so will find two matches: one data, one xml.

        :param tile:
        :param archive_address: the parent archive web address
        :return: link names or empty array
        """

        retval = []
        search_terms = [website_const.data_file_ext, tile]
        with open(self.m_web_text_file_name) as f:
            for line in f:
                if all(x in line for x in search_terms):
                    link = line.split()[website_const.line_split_1].split('\"')[website_const.line_split_2]
                    # this gives us the relative path, need to prepend http:\\ parent part
                    retval.append(archive_address + '/' + link)
                    if len(retval) == website_const.data_files_to_retrieve: # don't continue to search the text once the matches are found
                        break
        f.close()
        return retval
